{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Work] Fetching current semester ......\n",
      "[Done] Current semester: 1122\n",
      "[Work] Fetching departments ......\n",
      "Parsing Chinese department data ......\n",
      "Parsing Chinese department data ......\n",
      "[Done] All departments fetched ......\n",
      "Sch: 1 / 77 department(s)\n",
      "[Work] Fetching departments ......\n",
      "Target department: Physical Education Office\n",
      "Found 5 pages of courses ......\n",
      "Parsing Chinese course data ......\n",
      "Parsing English course data ......\n",
      "[Done] All departments fetched ......\n",
      "Sch: 2 / 77 department(s)\n",
      "[Work] Fetching departments ......\n",
      "Target department: Military Training Office\n",
      "Found 1 pages of courses ......\n",
      "Parsing Chinese course data ......\n",
      "Parsing English course data ......\n",
      "[Done] All departments fetched ......\n",
      "Sch: 3 / 77 department(s)\n",
      "[Work] Fetching departments ......\n",
      "Target department: Division of Student Affairs-Student Counselling\n",
      "Found 1 pages of courses ......\n",
      "Parsing Chinese course data ......\n",
      "Parsing English course data ......\n",
      "[Done] All departments fetched ......\n",
      "Sch: 4 / 77 department(s)\n",
      "[Work] Fetching departments ......\n",
      "Target department: General Education Center\n",
      "Found 3 pages of courses ......\n",
      "Parsing Chinese course data ......\n",
      "Parsing English course data ......\n",
      "[Done] All departments fetched ......\n",
      "Sch: 5 / 77 department(s)\n",
      "[Work] Fetching departments ......\n",
      "Target department: Language Center\n",
      "Found 3 pages of courses ......\n",
      "Parsing Chinese course data ......\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSch:\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(departments),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartment(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     dep  \u001b[38;5;241m=\u001b[39m departments[index]\n\u001b[0;32m---> 23\u001b[0m     courses \u001b[38;5;241m=\u001b[39m \u001b[43mcraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_courses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     all_courses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m courses\n\u001b[1;32m     25\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(courses, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all_course.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m),ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/NCU_course_selection_system_API/src/crawler/craw.py:64\u001b[0m, in \u001b[0;36mfetch_courses\u001b[0;34m(department_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(page_links)):\n\u001b[1;32m     63\u001b[0m     response \u001b[38;5;241m=\u001b[39m reqter\u001b[38;5;241m.\u001b[39mgetter(page_links[page_index])\n\u001b[0;32m---> 64\u001b[0m     ch_result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mparse_course_ch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m reqter\u001b[38;5;241m.\u001b[39mtoggle_language()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParsing English course data ......\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NCU_course_selection_system_API/src/crawler/crawparser.py:76\u001b[0m, in \u001b[0;36mparse_course_ch\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_course_ch\u001b[39m(response):\n\u001b[0;32m---> 76\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[43mbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     main_tb \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     78\u001b[0m     courses \u001b[38;5;241m=\u001b[39m main_tb\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/html/parser.py:172\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    170\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_starttag(i)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m--> 172\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<!--\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m    174\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_comment(i)\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/html/parser.py:420\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cdata_mode()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:176\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malready_closed_empty_element\u001b[38;5;241m.\u001b[39mremove(name)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/__init__.py:770\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Called by the tree builder when an ending tag is encountered.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m:param name: Name of the tag.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m:param nsprefix: Namespace prefix for the tag.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popToTag(name, nsprefix)\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/__init__.py:618\u001b[0m, in \u001b[0;36mBeautifulSoup.endData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    616\u001b[0m containerClass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_container(containerClass)\n\u001b[1;32m    617\u001b[0m o \u001b[38;5;241m=\u001b[39m containerClass(current_data)\n\u001b[0;32m--> 618\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_was_parsed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/__init__.py:646\u001b[0m, in \u001b[0;36mBeautifulSoup.object_was_parsed\u001b[0;34m(self, o, parent, most_recent_element)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Check if we are inserting into an already parsed node.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linkage_fixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crawler/lib/python3.9/site-packages/bs4/__init__.py:648\u001b[0m, in \u001b[0;36mBeautifulSoup._linkage_fixer\u001b[0;34m(self, el)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linkage_fixer(parent)\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_linkage_fixer\u001b[39m(\u001b[38;5;28mself\u001b[39m, el):\n\u001b[1;32m    649\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make sure linkage of this fragment is sound.\"\"\"\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     first \u001b[38;5;241m=\u001b[39m el\u001b[38;5;241m.\u001b[39mcontents[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from crawler import craw\n",
    "import datetime\n",
    "import os \n",
    "import json \n",
    "\n",
    "current_semester = craw.fetch_semester()\n",
    "output_path = f\"../api/{current_semester}\"\n",
    "\n",
    "initial_fetch = False\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    initial_fetch = True\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "colleges,departments = craw.fetch_departments()\n",
    "json.dump(colleges,    open(f\"{output_path}/colleges.json\",    \"w\"),ensure_ascii=False)\n",
    "json.dump(departments, open(f\"{output_path}/departments.json\", \"w\"),ensure_ascii=False)\n",
    "all_courses = []\n",
    "\n",
    "for index in range(len(departments)):\n",
    "    print(f\"On fetching department's courses, current->({index+1}/{len(departments)})\")\n",
    "    dep  = departments[index]\n",
    "    courses = craw.fetch_courses(dep)\n",
    "    all_courses += courses\n",
    "json.dump(courses, open(f\"{output_path}/all_course.json\", \"w\"),ensure_ascii=False)\n",
    "\n",
    "failed = []\n",
    "fetched = set()\n",
    "for index in range(len(all_courses)):\n",
    "    course = all_courses[index]\n",
    "    print(f\"On fetching course detail, current->({index+1}/{len(all_courses)})\")\n",
    "    initial_fetch = False\n",
    "    if course['serial'] in fetched:\n",
    "        print(f\"[Warning] Course serial:{course['serial']} has been fetched\")\n",
    "        continue\n",
    "    try:\n",
    "        course_detail = craw.fetch_course_detail(course)\n",
    "        fetched.add(course['serial'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"[Fetal Error] Failed to fetch course detail for serial:{course['serial']}\")\n",
    "        course['error_msg'] = str(e)\n",
    "        failed.append(course)\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(f\"{output_path}/{course['serial']}\"):\n",
    "        os.makedirs(f\"{output_path}/{course['serial']}\")\n",
    "        os.makedirs(f\"{output_path}/{course['serial']}/history\")\n",
    "        initial_fetch = True\n",
    "        statistics = {'timestamp':[datetime.datetime.now().timestamp()], \n",
    "                'data'     :{'selected':[course_detail['selected']],\n",
    "                             'assigned':[course_detail['assigned']],\n",
    "                             'preselecStu':[course_detail['preselecStu']],\n",
    "                             'stuGender':[course_detail['stuGender']],}\n",
    "        }\n",
    "        json.dump(statistics, open(f\"{output_path}/{course['serial']}/history/statistics.json\", \"w\"),ensure_ascii=False)\n",
    "\n",
    "    if not initial_fetch:\n",
    "        last_statistics = json.load(open(f\"{output_path}/{course['serial']}/history/statistics.json\"))\n",
    "        last_statistics['timestamp'].append(datetime.datetime.now().timestamp())\n",
    "        last_statistics['data']['selected'].append(course_detail['selected'])\n",
    "        last_statistics['data']['assigned'].append(course_detail['assigned'])\n",
    "        last_statistics['data']['preselecStu'].append(course_detail['preselecStu'])\n",
    "        last_statistics['data']['stuGender'].append(course_detail['stuGender'])\n",
    "        json.dump(last_statistics, open(f\"{output_path}/{course['serial']}/history/statistics.json\", \"w\"),ensure_ascii=False)\n",
    "    json.dump(course_detail, open(f\"{output_path}/{course['serial']}/detial.json\", \"w\"),ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "    # stu_list history name (current date)\n",
    "    his_name = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    json.dump(course_detail['stus'], open(f\"{output_path}/{course['serial']}/history/{his_name}.json\", \"w\"),ensure_ascii=False)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"=====Generating fetch status report=====\")\n",
    "est_total_course = 0\n",
    "for dep in departments:\n",
    "    est_total_course += dep['course_cnt']\n",
    "status = {'update_time':datetime.datetime.now().strftime('%Y-%m-%d-%H-%M'),\n",
    "          'total_colleges'   :len(colleges),\n",
    "          'total_departments':len(departments), \n",
    "          'estimated_total_courses':est_total_course,\n",
    "          'total_courses'    :len(all_courses),\n",
    "          'detail_actual_fetched'   :len(fetched),\n",
    "          'duplicate'        :len(all_courses) - len(fetched),\n",
    "          'failed'           :len(failed)}\n",
    "\n",
    "json.dump(status, open(f\"{output_path}/status.json\", \"w\"),ensure_ascii=False)\n",
    "json.dump(failed, open(f\"{output_path}/failed.json\", \"w\"),ensure_ascii=False)\n",
    "print(\"================Work Done===============\")\n",
    "print(\"================ reports ===============\")\n",
    "# print reports\n",
    "print(f\"Total colleges          :{len(colleges)}\")\n",
    "print(f\"Total departments       :{len(departments)}\")\n",
    "print(f\"Estimated total courses :{est_total_course}\")\n",
    "print(f\"Total courses           :{len(all_courses)}\")\n",
    "print(f\"Detail actual fetched   :{len(fetched)}\")\n",
    "print(f\"Duplicate               :{len(all_courses) - len(fetched)}\")\n",
    "print(f\"Failed                  :{len(failed)}\")\n",
    "print(\"========================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
